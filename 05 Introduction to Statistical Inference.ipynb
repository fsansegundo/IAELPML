{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41fe488",
   "metadata": {},
   "source": [
    "### <img src='./fig/vertical_COMILLAS_COLOR.jpg' style= 'width:70mm'>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "Máster en Big Data. Tecnología y Analítica Avanzada (MBD).\n",
    "<a class=\"tocSkip\">\n",
    "</h1>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "Introducción al Análisis Estadístico con Lenguajes de Programación para Machine Learning (IAELPML). 2023-2024.\n",
    "</h1>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "05 Introduction to Statistical Inference\n",
    "<a class=\"tocSkip\">   \n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532748f",
   "metadata": {},
   "source": [
    "## <span style='background:yellow; color:red'> Remember:<a class=\"tocSkip\"> </span>     \n",
    "\n",
    "+ Navigate to your `fmad2223` folder in the console/terminal.  \n",
    "+ Execute `git pull origin main` to update the code\n",
    "+ **Do not modify the files in that folder**, copy them elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98b151",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard Data Science Libraries Import\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "#sns.set(rc={'figure.figsize':(12, 8.5)})\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa25e1",
   "metadata": {},
   "source": [
    "## Central Limit Theorem and the Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc8ed9",
   "metadata": {},
   "source": [
    "### Precise Statement of the CLT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c7704",
   "metadata": {},
   "source": [
    "+ In previous sessions we have seen evidence that the sample means of several quite different types of initial populations were in all cases normally distributed. And now that we have a bit more of experience with the normal distribution we are in a position to state that result formally and to begin making use of it to do inference. \n",
    "\n",
    "+ So let us recall the context of the CLT. We want to understand the distribution of a certain quantitative random variable $X$. Let us use the symbols $\\mu$ and $\\sigma$ for the mean and standard deviation of $X$ respectively, as usual. \n",
    "\n",
    "+ We will try to estimate the value of $\\mu$ by taking samples of $n$ values of this variable: \n",
    "$$\n",
    "\\quad\\\\\n",
    "x_1,\\, x_2,\\,\\ldots,\\, x_n\n",
    "\\quad\\\\\n",
    "$$\n",
    "We are talking here about sampling at random and with replacement, so that we have **simple random sampling**. Then we can use the *sample mean*\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\bar x = \\dfrac{x_1 + x_2 + \\cdots + x_n}{n}\n",
    "\\quad\\\\\n",
    "$$\n",
    "to try to estimate $\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec7d03",
   "metadata": {},
   "source": [
    "+ **The sample space.** It is also important to recall that when considering the sample space (the set of all possible simple random samples) we are moving into a huge set even when the original population size and the sample size are modest. With a population of $1000$ individuals and size $n = 7$ samples we are looking at a sample space consisting of\n",
    "$$\n",
    "1000^7 = 1000000000000000000000\n",
    "$$\n",
    "different samples. These samples can turn out to be *bad* or *good* for our intent of estimating $\\mu$. That is, a sample is *good* if the value of $\\bar x$ in that sample is close to $\\mu$, and *bad* if the value of $\\bar x$ is misleading. To really understand the sampling process we need to understand the **distribution of the sample mean in the sample space**. That is precisely what the CLT does.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28929daf",
   "metadata": {},
   "source": [
    "+ **Statement of the Central Limit Theorem** Let $X$ be a random variable with mean $\\mu$ and variance $\\sigma^2$. Let $\\bar X$ be the random variable describing the simple random sampling process. That is\n",
    "$$\\bar X = \\dfrac{X_1+X_2+\\cdots +X_n}{n}$$\n",
    "where $X_1, \\ldots, X_n$ are *independent copies of $X$*.\n",
    "\n",
    "\n",
    "**Central Limit Theorem**\n",
    "\n",
    "When we consider large enough sample sizes $n$ the distribution of the sample mean in the sample space is\n",
    "$$\n",
    "\\bar X \\sim N\\left(\\mu_X,\\frac{\\sigma}{\\sqrt{n}}\\right) \n",
    "$$\n",
    "What do we mean by *large enough?* The answer depends on the original population. If e.g. the original population is normal, then any sample size will do (even $n = 1$). But if you start with a highly asymmetrical population then the *large enough* value of $n$ may turn out to be quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8b992",
   "metadata": {},
   "source": [
    "# Confidence Intervals for the Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6703f6",
   "metadata": {},
   "source": [
    "## Intervals as Estimates\n",
    "\n",
    "+ We begin with a simple case. Let us suppose that the variable $X$ is approximately normal, but its mean $\\mu$ is unknown and we are trying to estimate it through sampling. This is a frequent situation because there are many variables in the real world that tend to be (approximately) normally distributed.\n",
    "\n",
    "+ **Random Nature of the Sampling Error:** Let us suppose that we are taking a sample size $n$ large enough, so that the TCL applies for samples of size $n$. Then we can take a simple random sample and estimate $\\mu \\approx \\bar X$. That means, of course:\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\mu = \\bar X \\pm \\text{error}\n",
    "\\quad\\\\\n",
    "$$\n",
    "At this point this is not very formal, but it is very important to understand early on that **the error term that appears here depends on the sample and is therefore a random error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115e503",
   "metadata": {},
   "source": [
    "+ To make the statement above scientifically useful we need to quantify the size of the error. If we discover that the size of the error is smaller than $\\delta$ (think of $\\delta$ as a small number) then we can say that:\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\bar X - \\delta < \\mu < \\bar X + \\delta\n",
    "\\quad\\\\\n",
    "$$\n",
    "which means that we are thinking of an estimate in the shape of an interval $(a, b) =  (\\bar X - \\delta, \\bar X + \\delta)$. Next we will be using the TCL to get a precise description of this interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a511799",
   "metadata": {},
   "source": [
    "### The error is random!\n",
    "\n",
    "+ We want to emphasize this because it is a central but often misunderstood idea in the foundations of Statistical Inference. The following figure (the code to generate it is in an external script) shows 20 different samples (of size $n = 30$) of the same approximately normal population. The red vertical dashed line indicates the mean of the population, which in this example is $\\mu = 0$. The (solid blue) points from each of the sample are placed at the same horizontal lines. The mean for each sample is indicated with a red cross. It is extremely important to understand that these are all well formed samples, they were all obtained through the same sampling procedure. Yet, the quality of the information that they provide about $\\mu$ is very different from one to the next. There are samples whose mean is really close to $\\mu$ but some others (like sample number 2 in this case) have sample means that deviate more. This is what we mean when we say that the error is random. In a real situation you would not now were the red line is and you would have no way to assess the quality of a single sample just by looking at it. We just have to play with the cards that we have been dealt!  \n",
    "\n",
    "  **Note:** the Python code to obtain the figure below is a bit more complicated, so it has been moved to an external script that you can find in the `code` folder of the repository. Take this chance also to learn how to call external scripts from a notebook by using the [*magic* commands from IPython](https://ipython.readthedocs.io/en/stable/interactive/magics.html) such as `%run` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"code/05-inference-01.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4eaddc",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "+ If we obtain a \"*good*\" sample then the error (when estimating $\\mu$ with $\\bar X$) will be small but if we get a \"*bad*\" enough sample the error can turn out to be quite large. The CLT guarantees that most of the random samples will be good in this particular sense. \n",
    "\n",
    "+ Remember once again that this distinction we are making between good and bad samples has nothing to do with the quality of the sampling process. A flawless sampling can lead to a very bad sample, just by chance. In order to make this process quantifiable we want to measure the **probability of coming across a bad sample**. And that is the reason why our interval estimation will take a probabilistic form.  \n",
    "\n",
    "**Confidence Intervals**\n",
    "\n",
    "Let $cl$ be a confidence level (as probability or percent). An interval $(a, b)$ such that \n",
    "\n",
    "$$P\\left(a < \\mu < b\\right) = cl $$\n",
    "is a **confidence interval at level $cl$**  for $\\mu$\n",
    "\n",
    "+ The key in the above definition is that **the probability is measured over the (frequently huge in size) sample space**, consisting of all the simple random $n$-sized samples from the population. Then $cl$, the confidence level, indicates the probability of randomly getting a good sample. We can now begin to make this idea of a *good sample* more precise. We will see below how every sample can be used to obtain an interval estimate $(a, b)$ for the mean. The sample is *good* if $a < \\mu < b$. The level of confidence measures the probability that the *interval from sample* method will lead to an interval that contains the mean. Again: it is an *statement about the method* or algorithm, it is *not a statement about any particular interval that the algorithm produces*. That is why we will always use high values of confidence level such as 95% or 99%, to make sure that there is a low probability that (we get a bad sample and) we obtain an interval not containing the true mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066dc8e",
   "metadata": {},
   "source": [
    "## How to obtain the confidence interval for $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8eaa7",
   "metadata": {},
   "source": [
    "+ Let us see the recipe for cooking confidence intervals.\n",
    "\n",
    "+ **First ingredient:** For a normal population (or very approximately normal provided the sample size $n$ is large enough)  the CLT states that\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\bar X \\sim N\\left(\\mu_X,\\frac{\\sigma}{\\sqrt{n}}\\right) \n",
    "\\quad\\\\\n",
    "$$\n",
    "and in particular this implies that\n",
    "$$\n",
    "\\quad\\\\\n",
    "Z = \\dfrac{\\bar X - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n",
    "\\quad\\\\\n",
    "$$\n",
    "is a standard normal $Z = N(0, 1)$.\n",
    "\n",
    "+ **Second ingredient:** Given a confidence level $cl$, considered as a close-to-one probability, we already know how to obtain a symmetrical interval $(-K, K)$ such that:\n",
    "$$\n",
    "\\quad\\\\\n",
    "P(-K < \\, Z \\,<  K) = cl\n",
    "\\quad\\\\\n",
    "$$\n",
    "as illustrated in the picture below.\n",
    "![](fig/06-02-ProblemaInversoZ-02.png)\n",
    "Let us give $K$ a name. The shaded area in the figure is $cl$. That means that the probability left in *both tails together of Z* is \n",
    "$$\\alpha = 1 - cl$$\n",
    "In particular, due to the symmetry, each one of the tails has area equal to $\\dfrac{\\alpha}{2}$.\n",
    "\n",
    "**Critical values for $Z$**\n",
    "\n",
    "Given a probability $p$,  the  **critical value** $z_p$ is the value of $Z$ for which the probability of the **right tail** equals $p$. That is:\n",
    "$$\n",
    "P\\left(Z > z_p\\right) = p \n",
    "$$\n",
    "\n",
    "In particular, $K = z_{\\alpha/2}$ in the previous figure\n",
    "$$\n",
    "P\\left(-z_{\\alpha/2} < \\, Z \\,<  z_{\\alpha/2}\\right) = cl = 1 - \\alpha\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890fd17",
   "metadata": {},
   "source": [
    "+ Now the recipe is very simple: substitute the expression for $Z$ in the first ingredient into this last expression from the second ingredient, and do some very simple algebra with the inequalities to arrive at the following:\n",
    "\n",
    "**Preliminary expression for the confidence interval**\n",
    "\n",
    "A confidence interval $(a, b)$ for $\\mu$ with a confidence level $cl$ is provided by:\n",
    "$$\n",
    "a = \\bar X - z_{\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}}, \\qquad\\qquad \n",
    "b = \\bar X + z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "Or equivalently:\n",
    "$$\n",
    "\\hspace{4cm}\\mu = \\bar X \\pm z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "\n",
    "+ **Why is this a preliminary expression?** Because we want a method to obtain the interval using the information from the sample, without additional information about the population parameters. And this expression uses $\\sigma$, which is a population (therefore unknown) parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8d559",
   "metadata": {},
   "source": [
    "### The Large Samples Approximation..\n",
    "\n",
    "+ To overcome this problem of the unknown value of $\\sigma$ we will assume that population is normal (or very approximately normal) and that the sample size $n$ is large enough. If this is the case, then we can replace $\\sigma$ by $s$, the standard deviation of the mean to get at our first really useful formula for a confidence interval:\n",
    "\n",
    "**Confidence interval for the mean, normal population, large sample.**\n",
    "\n",
    "A confidence interval $(a, b)$ for $\\mu$ with a confidence level $nc$ is provided by:\n",
    "$$\n",
    "\\mu = \\bar X \\pm z_{\\alpha/2}\\dfrac{s}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "+ What do we mean by **large sample**? Traditionally $n \\geq 30$ was considered large enough for normal populations. But to get more accurate results we recommend taking at least $n \\geq 100$. This sample size helps the effects of the CLT kick in.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf7194",
   "metadata": {},
   "source": [
    "+ **Example:** A sample from a normal population has these *sample values*:\n",
    "$$\n",
    "n = 100,\\quad \\bar X = 7.34,\\quad s= 0.31\n",
    "$$\n",
    "Let $cl = 0.95$ (thus $\\alpha = 1- cl = 0.05$). Let us use these values to obtain a 95% confidence interval for the mean $\\mu$ of the population from which this sample originated. We begin by using Python to find the critical point\n",
    "$$z_{\\alpha/2} = z_{0.025}$$\n",
    "We already did this as exercise S04-006 in the previous session, so let us just repeat the code here (and read that part of the session if you need to refresh your ideas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee10058",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 0.95\n",
    "alpha = 1 - cl # both tails\n",
    "print(\"The two tails should add up to: {:.3}\".format(alpha))\n",
    "prob_1_tail = alpha / 2\n",
    "print(\"Therefore each tail has probability = {:.3}\".format(prob_1_tail))\n",
    "print(\"But the ppf function in NumPy works with the left tail and so we \") \n",
    "print(\"need to find the probability of the left tail of the critical point, which is\")\n",
    "prob_left_tail_crit_point = 1 - prob_1_tail\n",
    "print(\"{:.3}\".format(prob_left_tail_crit_point))\n",
    "print(\"Now we can use ppf to find the critical point for this confidence level:\")\n",
    "crit_point = stats.norm.ppf(prob_left_tail_crit_point, loc = 0, scale = 1)\n",
    "print(\"{:.6}, approximately  {:.3}\".format(crit_point, crit_point))\n",
    "print(\"Alternative using the inverse survival function:\")\n",
    "crit_point = stats.norm.isf(prob_1_tail, loc = 0, scale = 1)\n",
    "print(\"{:.6}, approximately  {:.3}\".format(crit_point, crit_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69d175",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "+ Now we can plug this value for $z_{\\alpha/2}$ and the rest of the sample values in the expression for the confidence interval and we arrive at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182968fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "barX = 7.34\n",
    "s = 0.31\n",
    "\n",
    "# Short version of the critical point computation:\n",
    "cl = 0.95\n",
    "alpha = 1 - cl\n",
    "crit_point = stats.norm.isf(alpha/2, loc = 0, scale = 1)\n",
    "\n",
    "conf_int = barX + np.array([-1, 1]) * crit_point * s / np.sqrt(n)\n",
    "print(\"The confidence interval is \", conf_int)\n",
    "print(\"Rounded to 4 digits: [{:.4}, {:.4}]\".format(conf_int[0], conf_int[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e246e",
   "metadata": {},
   "source": [
    "+ **Exercise (S05-001):** Using the data in the `05-Conf_Interval_LargeSample.csv` file (you sholud already have this in your `data` subfolder of the `fmad2223` folder if you keep the repository updated) find a 95% confidence interval for the mean of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S05-001.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c61e5",
   "metadata": {},
   "source": [
    "## Back to the Random Nature of Estimation\n",
    "\n",
    "+ Now that we have a working formula for the confidence interval we will go back to the previous example of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df746bfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run \"code/05-inference-02.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477274b6",
   "metadata": {},
   "source": [
    "## Determining the Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bbd33a",
   "metadata": {},
   "source": [
    "+ It is very important to understand that the construction of the confidence interval involves two different sources of uncertainty:\n",
    "\n",
    "    1. The width of the symmetrical interval $(a, b) = \\mu\\pm\\delta$ is related to the **precision** (or conversely the error) in our estimate of $\\mu$ (in $X$ original units). Thus, the narrower the interval the better. \n",
    "    \n",
    "    2. But the **confidence** level $cl$ measures the (sample space) probability of that estimation, which depends on how lucky we were in the sampling step. Thus, values of $cl$ closer to 1 are better. \n",
    "    \n",
    "  Unfortunately, precision and confidence are not independent and in practice we need to find a balance between aiming at a higher precision and working with high confidence levels \n",
    "  \n",
    "+ It is intuitively clear that if we want more precision we should try to get larger samples. Recall that the preliminary expression for the confidence interval said:\n",
    "$$\n",
    "\\mu = \\bar X \\pm z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "This implies that the precision (half-width) of the confidence interval is:\n",
    "$$\n",
    "\\delta = z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "Therefore, if we wish to obtain an interval with a precision such that $\\delta < 0.0001$ we can try to solve the above for $n$ and we arrive at this expression for the required sample size:\n",
    "$$\n",
    "z_{\\alpha/2}\\cdot\\dfrac{\\sigma}{\\sqrt{n}} < \\delta \\qquad \\Rightarrow \\qquad \n",
    "n > \\left(\\rule{0cm}{0.8cm}z_{\\alpha/2}\\cdot\\dfrac{\\sigma}{\\delta}\\right)^2\n",
    "$$\n",
    "But since $\\sigma$ is unknown we usually conduct a *pilot study* with a comparatively small sample size and use it to estimate $\\sigma$ by means of $s$. And then we use the above expression replacing $\\sigma$ with the $s$ value from the sample study.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c578eea",
   "metadata": {},
   "source": [
    "+ **Example:** We wish to estimate the mean diameter of some machinery parts produced at a factory. We assume that the diameter of those parts follows very approximately a normal distribution. A pilot experiment concluded tha the standard deviation of the parts diameter was $s = 1.3$mm. The factory managers require an estimate of the diameter with an error no bigger than $0.1$mm and a confidence level of $99\\%.$ What sample size do we need to achieve that goal?  \n",
    "Thus we want $\\delta=0.1$mm and $nc=0.99$. Let us use Python to get the required critical value and the minimum sample size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b24917",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "s = 1.3\n",
    "\n",
    "# Critical point computation:\n",
    "cl = 0.99\n",
    "alpha = 1 - cl\n",
    "crit_point = stats.norm.isf(alpha/2, loc = 0, scale = 1)\n",
    "print(\"The critical point for cl = \",cl, \" is approx. {:.4}\".format(crit_point))\n",
    "\n",
    "min_sample_size = np.ceil((crit_point * s / delta)**2)\n",
    "\n",
    "print(\"The minimum sample size is at least n = \", min_sample_size.astype(int), \n",
    "      \"(but possibly bigger).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1b426",
   "metadata": {},
   "source": [
    "+ Always keep in mind that in examples like this the results depend on the reliability of the pilot study, and so we must err on the side of caution, rounding up to larger values of $n$.\n",
    "\n",
    "+ Look at the above expression for the sample size:\n",
    "$$\n",
    "n > \\left(\\rule{0cm}{0.8cm}z_{\\alpha/2}\\cdot\\dfrac{\\sigma}{\\delta}\\right)^2\n",
    "$$\n",
    "as you can see $z_{\\alpha/2}$ is in the numerator, while $\\delta$ is in the denominator. In particular that means that if we keep the sample size $n$ fixed we can not increase precision (make $\\delta$ smaller) without making $z_{\\alpha/2}$ closer to the origin, which means working at a lower confidence level. This is an example of the interdependence between precision and confidence that we mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a42e",
   "metadata": {},
   "source": [
    "+ **Exercise (S05-002):** In a previous exercise you used the data from `05-Conf_Interval_LargeSample.csv` to get a confidence interval for the mean using the sample in that file. What was the width of that interval? What sample size would you need to find a 95% confidence interval with a precision $\\delta = 10^{-3}$. Use Python to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0dc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S05-002.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4438df7",
   "metadata": {},
   "source": [
    "# The Case of Small Samples in Normal Populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d7625",
   "metadata": {},
   "source": [
    "+ Our previous results about confidence intervals ultimately relied on the sample size being large enough to justify using $s$ as a replacement for the unknown population parameter $\\sigma$. What should we do if we know that the population is very approximately normal but we only manage to get a small sample of size e.g. $n = 15$?\n",
    "\n",
    "+ As the sample gets smaller we have less and less information about the population. And this clearly implies that in a wider (less precise) confidence interval. In particular, that means that the critical points provided by the normal are wrong, because they are too close to the origin to account for this loss of precision. In the turn of the 20th century the statistician W. Gosset, known by his pseudonym *Student*, figured out the necessary corrections to the normal distribution and created a family of distributions to obtain the right choice of critical values. These distributions are known as the **Student's $t$ family of distributions**, and there is one such Student's $t$ for each sample size.\n",
    "\n",
    "+ The Student's $t$ distributions are bell-shaped and symmetrical with mean 0, much like the standard normal $Z$, but the main difference is that the Student's $t$ have so-called **heavier tails**, meaning that a larger chunck of the total probability under the curve is shifted or squeezed towards infinity. The following figure illustrates this for a concrete choice of Student's $t$. You can see that the tails of the Student's $t$ are above those of $Z$ as we move towards $\\infty$ (and correspondingly, Student's $t$ has less probability concentrated in the central region close to 0).\n",
    "![](./fig/06-04-TvsZ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import IFrame\n",
    "#IFrame(\"https://www.geogebra.org/m/xegwa6cr\",800,800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fec1d",
   "metadata": {},
   "source": [
    "### Degrees of Freedom and Sample Size\n",
    "\n",
    "+ We have already said that there is a different Student's $t$ for each sample size. The relationship between the sample size and Student's $t$ uses the notion of *degrees of freedom (dof)* where:\n",
    "$$\n",
    "dof = n - 1\n",
    "$$\n",
    "for a sample of size $n$. That is, when we work with samples of size $n$ the appropriate critical points are provided by the Student's $t$ with $dof = n - 1$. If, for example we have $n = 10$ then we must use the Student's distribution called $t_9$, where the symbol $t_{dof}$ is used to identify the degress of freedom.\n",
    "\n",
    "+ Recall that we saw before that for large enough samples the use of Student's $t$ is not necessary and we can fall back to $Z$ and the critical points it provides. This in turn is equivalent to saying that for large enough $n$ the density curve of $Z$ and $t_{n - 1}$ are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ba3af",
   "metadata": {},
   "source": [
    "## Confidence Intervals Using Student's $t$\n",
    "\n",
    "+ The main contribution of Student's $t$ to the theory of small sample estimation is a new collection of critical points\n",
    "\n",
    "**Critical values for Student's $t$**\n",
    "\n",
    "Given a sample size  $n$ and a probability $p$, the **critical value** $t_{k;p}$(where $k = n - 1$)\n",
    "is the value of $t_{n - 1}$ for which the probability of the **right tail**  equals $p$. That is:\n",
    "$$\n",
    "P\\left(t_{k;p} > z_p\\right) = p \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9014e",
   "metadata": {},
   "source": [
    "+ The critical points are all we needed in order to obtain this expression:\n",
    "\n",
    "**Confidence interval for the mean, normal population, small samples using Student's $t$.**\n",
    "\n",
    "For a small sample size $n$ a confidence interval $(a, b)$ for $\\mu$ with a confidence level $nc$ is provided by:\n",
    "\n",
    "$$\\mu = \\bar X \\pm t_{n - 1;\\alpha/2}\\dfrac{s}{\\sqrt{n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d6376",
   "metadata": {},
   "source": [
    "### Student's $t$ in Python\n",
    "\n",
    "+ Working with $t_k$ in Python is easy with the help of [scipy.stats.t](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html). Take a look at the documentation and the methods provided before moving on to the examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b094f84",
   "metadata": {},
   "source": [
    "+ **Exercise (S05-003):** we suspect that the waters of a reservoir have $NO_2$ levels higher than the safe threshold for aquatic life. To study this problem we test the water concentration of $NO_2$ (mg/l) at 10 different points at random in the reservoir and we obtained these values:  \n",
    "```\n",
    "        0.04, 0.05, 0.03, 0.06, 0.04, 0.06, 0.07, 0.03, 0.06, 0.02\n",
    "```        \n",
    "Use `scipy.stats.t` and find a 95% confidence level interval for the mean $NO_2$ concentration in the water of the reservoir. The danger level for $NO_2$ is 0.03mg/l. What is your conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73671b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S05-003.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d901f26",
   "metadata": {},
   "source": [
    "+ **Exercise (S05-004):** Using the variable `gear_ratio` variable from the `auto2` data set, find a 95% confidence interval for the mean of that variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78677c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S05-004.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a8925",
   "metadata": {},
   "source": [
    "### The Remaining Case: Non Normal  Populations\n",
    "\n",
    "+ When the population is definitely not normal then in order to obtain confidence intervals for the mean we should turn to other methods, in particular the family of **non parametric methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fec7c",
   "metadata": {},
   "source": [
    "# Confidence Intervals for the Variance in Normal Populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a75aaa",
   "metadata": {},
   "source": [
    "+ After dealing with $\\mu$ the obvious natural step is to ask ourselves about $\\sigma^2$, the population variance. Our starting point is again the case of (at least approximately) normal populations, in order to keep our work under the protection of the CLT and similar results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45e13a",
   "metadata": {},
   "source": [
    "+ The first natural idea, when working with a normal random variable $X\\sim N(\\mu, \\sigma)$ is to try to estimate $\\sigma^2$ using the value of $s^2$ in a random sample of the population. In order for this idea to become useful we need something like the description of the sample distribution of the mean provided by the CLT, but in this case for $s^2$ instead of $\\bar X$.\n",
    "\n",
    "+ Before getting into the technical details keep in mind that $\\mu$ is a *position or center measure* and therefore we are interested in deviations from $\\mu$ in the form of differences $\\mu - \\bar X $. Meanwhile, $\\sigma^2$ is a measure of *spread* or *dispersion*. Therefore it is not the differences that matter, but the *quotients*. Think about that: when we compare two populations in terms of dispersal we usually make statements such as \"this population is **twice** as spread as the other\". Dispersion has a multiplicative nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ae047",
   "metadata": {},
   "source": [
    "+ Keeping that in mind, the sampling distribution result we require is this:\n",
    "\n",
    "**Sampling Distribution of $\\sigma^2$ in normal populations.**\n",
    "\n",
    "If $X\\sim N(\\mu, \\sigma)$ and we consider samples of size $n$ then:\n",
    "$$\n",
    "(n - 1)\\dfrac{s^2}{\\sigma^2}\\sim\\chi^2_{n - 1}\n",
    "$$\n",
    "where $\\chi^2_{n - 1}$ is the }\\textbf{chi-squared distribution with $dof = n - 1\\quad$. Let us see what this chi-squared distributions look like. \n",
    "\n",
    "+ Just like the Student's $t$ (to which it is related) the Chi-squared distribution is indeed a family of distributions, one for each sample size. It is however different in that it **only takes positive values and it is highly asymmetrical.** The typical plot of the density curve for one of these Chi-squared distributions is:\n",
    "![](fig/06-06-DensidadChiCuadrado.png)\n",
    "That is in fact the graph of $\\chi^2_4$. This asymmetry impacts as we will see in the construction of confidence intervals based on these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa03f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import IFrame\n",
    "#IFrame(\"https://www.geogebra.org/m/k4rvfbhf\",800,800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad1a19",
   "metadata": {},
   "source": [
    "### Chi-squared Distributions with Python\n",
    "\n",
    "+ Before moving further please take a look at the SciPy documentation for [scipy.stats.chi2](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html#scipy.stats.chi2). After your experience with the normal and Student's $t$ you should now be seeing a common pattern in the way that all these distributions are handled in the `scipy.stats` module. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6bcf3",
   "metadata": {},
   "source": [
    "## Confidence Interval Expression for the Variance\n",
    "\n",
    "+ The main difference in this case is that the lack of symmetry in the $\\chi^2$ distribution forces us to use two different critical points, one at each side of the distribution, as illustrated in this figure:\n",
    "![](fig/06-07-ChiCuadradoValoresCriticosIntervalo.png)\n",
    "\n",
    "</br></br>\n",
    "\n",
    "+ The **definition of the critical points** is what you expect:\n",
    "$$\n",
    "P(Y > \\chi^2_{k;p}) = p\n",
    "$$\n",
    "for any probability $p$ and $k = n - 1$ degrees of freedom. And with this we can state the expression:\n",
    "\n",
    "**Confidence interval for the variance in normal populations with sample size $n$.***\n",
    "$$\n",
    "\\dfrac{(n-1)s^2}{\\chi^2_{k,\\alpha/2}}\\leq\\sigma^2\\leq\\dfrac{(n-1)s^2}{\\chi^2_{k,1-\\alpha/2}}\n",
    "$$\n",
    "where $k = n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3d12c",
   "metadata": {},
   "source": [
    "+ **Exercise (S05-005):** a random variable $X$ is normally distributed. A random sample of $n= 7$ values of $X$ had $s^2 = 62$. Use Python to find a 95% confidence level interval for the variance $\\sigma^2$ of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541269c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S05-005.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4859b",
   "metadata": {},
   "source": [
    "+ **Exercise (S05-006):** Use Python to find a 95% confidence level interval for the variance $\\sigma^2$ of\n",
    "the variable `gear_ratio` variable from the `auto2` data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99371ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S05-006.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14f759",
   "metadata": {},
   "source": [
    "# Assessing Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a08678",
   "metadata": {},
   "source": [
    "+ A large part of the inference methods from classical Statistics rely on the assumption that the population under study is at least approximately normal. How can we check that assumption? In large samples we can look at histograms and density curves. The next figure shows on the left a sample of normal data (variable \"X1\") and on the right a sample of clearly non-normal data (variable \"X2\"), with sample size $n = 500$ in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "N = 500\n",
    "df = pd.DataFrame({\"x1\":np.random.normal(size = N),\n",
    "                  \"x2\":np.random.chisquare(df = 4, size = N)})\n",
    "\n",
    "df_long = df.melt()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 7]\n",
    "sns.displot(data = df_long, x = \"value\", col= \"variable\", kde = True, bins = 50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82ff45",
   "metadata": {},
   "source": [
    " + Some of the classical methods will work even when the population is not normal, as long as it is symmetric. We can check that in the above plots, but also with the help of boxplots. For the same two examples as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.loc[:, [\"x1\",\"x2\"]].plot.box(\n",
    "    subplots=True, layout=(1,2), sharex=False, sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a4b09",
   "metadata": {},
   "source": [
    "## QQplots\n",
    "\n",
    "+ The name comes from \"quantile vs quantile plots\" because the horizontal axis represents the quantiles or percentiles of a normal (the theoretical precise values) while the vertical axis contains the empirical percentiles of the sample. Thus, if the sample comes from a normal distribution the points should fall on a  line. See below for the two samples we have been using (normal to the left, non-normal to the right). This is probably the most widely used kind of plot to check the normality of a sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "ax1 = plt.subplot(121)\n",
    "sm.qqplot(df.x1, line=\"s\", ax=ax1)\n",
    "ax2 = plt.subplot(122)\n",
    "sm.qqplot(df.x2, line=\"s\", ax = ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45d978",
   "metadata": {},
   "source": [
    "## Normality for Small Samples.\n",
    "\n",
    "+ But for smaller samples things are not so clear-cut. The following series of plots shows the density curves for 12 different small samples of size $n = 10$ **of the same normal population!!** The plots are very different from one another and in many cases also clearly different from the normal density curve. This should serve as a warning about the dangers of small samples: assessing normality is very difficult, for starters. There are formal criteria to do that but in many cases those criteria simply provide a false feeling of safety, with little evidence supporting it. If you have a very small sample either use methods that do not depend on the hypothesis of normality or solve the normality issue by other means (expert knowledge, perhaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8091df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "n = 10\n",
    "N = 12 \n",
    "\n",
    "df = pd.DataFrame({\"x\":np.random.normal(size = N * n),\n",
    "                  \"sample\":np.repeat(np.arange(N), n)})\n",
    "\n",
    "df.head(25)\n",
    "plt.rcParams['figure.figsize'] = [15, 7]\n",
    "sns.displot(data = df, x = \"x\", col= \"sample\", col_wrap=4, kind=\"kde\", color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc591a",
   "metadata": {},
   "source": [
    "+ The same thing happens when using boxplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c06830",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "n = 10\n",
    "N = 12 \n",
    "\n",
    "df = pd.DataFrame(np.random.normal(size = (n, N)), columns=[\"col\" + str(_) for _ in range(N)]\n",
    "                 )\n",
    "df.plot.box(subplots=True, layout=(3, 4), sharex=False, sharey=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93532bf9",
   "metadata": {},
   "source": [
    "+ And similarly, trying to decide about normality using the qq-plot is very hard with so few data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c80b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "    sm.qqplot(df.iloc[:, i], line=\"s\", ax=ax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7d0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
